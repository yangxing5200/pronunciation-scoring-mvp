"""
Task 4: Acoustic Scoring - Enhanced Version

ä½¿ç”¨ WavLM åµŒå…¥å‘é‡å’Œä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œå£°å­¦è¯„åˆ†ã€‚
è¯„ä¼°å£°æ¯/éŸµæ¯çš„å‘éŸ³è´¨é‡ã€‚

å¢å¼ºåŠŸèƒ½ï¼š
- ä¼˜åŒ–ä¸æ ‡å‡†éŸ³çš„å¯¹æ¯”é€»è¾‘
- æ”¹è¿›æ— æ ‡å‡†éŸ³æ—¶çš„å¯å‘å¼è¯„åˆ†
- æ›´å¥½çš„çŸ­éŸ³é¢‘å¤„ç†
"""

from typing import List, Dict, Optional
import numpy as np
import warnings


class AcousticScorer:
    """
    åŸºäº WavLM åµŒå…¥çš„å£°å­¦è¯„åˆ†å™¨ã€‚
    
    é€šè¿‡æ¯”è¾ƒç”¨æˆ·éŸ³é¢‘ä¸å‚è€ƒéŸ³é¢‘çš„åµŒå…¥ç›¸ä¼¼åº¦æ¥è¯„ä¼°å‘éŸ³è´¨é‡ã€‚
    """
    
    # WavLM å¤„ç†çš„æœ€å°éŸ³é¢‘é•¿åº¦
    MIN_AUDIO_LENGTH = 512  # é‡‡æ ·ç‚¹ @ 16kHz (~32ms)
    
    # WavLM-base+ çš„åµŒå…¥ç»´åº¦
    EMBEDDING_DIM = 768
    
    def __init__(self, device: Optional[str] = None):
        """
        åˆå§‹åŒ–å£°å­¦è¯„åˆ†å™¨ã€‚
        
        Args:
            device: ä½¿ç”¨çš„è®¾å¤‡ ('cuda' æˆ– 'cpu')ï¼ŒNone æ—¶è‡ªåŠ¨æ£€æµ‹
        """
        self.device = device
        self.model = None
        self.processor = None
        self.available = False
        
        try:
            import torch
            if device is None:
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.torch = torch
        except ImportError:
            warnings.warn("PyTorch ä¸å¯ç”¨")
            return
        
        try:
            from transformers import Wav2Vec2FeatureExtractor, WavLMModel
            self.FeatureExtractor = Wav2Vec2FeatureExtractor
            self.WavLMModel = WavLMModel
            self.available = True
        except ImportError:
            warnings.warn(
                "Transformers ä¸å¯ç”¨ã€‚å®‰è£…å‘½ä»¤: pip install transformers"
            )
    
    def load_model(self, model_name: str = "microsoft/wavlm-base-plus"):
        """
        åŠ è½½ WavLM æ¨¡å‹ã€‚
        
        Args:
            model_name: HuggingFace æ¨¡å‹åç§°
        """
        if not self.available:
            raise RuntimeError("Transformers ä¸å¯ç”¨")
        
        print(f"ğŸ“¥ åŠ è½½ WavLM æ¨¡å‹: {model_name}")
        
        self.processor = self.FeatureExtractor.from_pretrained(model_name)
        self.model = self.WavLMModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… WavLM æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {self.device})")
    
    def _pad_audio_if_needed(self, audio_segment: np.ndarray) -> np.ndarray:
        """
        å¦‚æœéŸ³é¢‘è¿‡çŸ­ï¼Œè¿›è¡Œå¡«å……ã€‚
        
        Args:
            audio_segment: è¾“å…¥éŸ³é¢‘é‡‡æ ·ç‚¹
        
        Returns:
            å¡«å……åçš„éŸ³é¢‘
        """
        if len(audio_segment) < self.MIN_AUDIO_LENGTH:
            padding_length = self.MIN_AUDIO_LENGTH - len(audio_segment)
            return np.pad(audio_segment, (0, padding_length), mode='constant')
        return audio_segment
    
    def extract_embedding(self, audio_segment: np.ndarray) -> np.ndarray:
        """
        æå– WavLM åµŒå…¥å‘é‡ã€‚
        
        Args:
            audio_segment: éŸ³é¢‘é‡‡æ ·ç‚¹ï¼ˆnumpy æ•°ç»„ï¼‰
        
        Returns:
            åµŒå…¥å‘é‡ï¼ˆnumpy æ•°ç»„ï¼‰
        """
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ã€‚è¯·å…ˆè°ƒç”¨ load_model()")
        
        if len(audio_segment) == 0:
            return np.zeros(self.EMBEDDING_DIM)
        
        original_length = len(audio_segment)
        audio_segment = self._pad_audio_if_needed(audio_segment)
        
        if original_length < self.MIN_AUDIO_LENGTH / 2:
            warnings.warn(
                f"éŸ³é¢‘ç‰‡æ®µè¿‡çŸ­ ({original_length} é‡‡æ ·ç‚¹)ï¼Œå£°å­¦è¯„åˆ†å¯èƒ½ä¸å‡†ç¡®ã€‚"
            )
        
        try:
            inputs = self.processor(
                audio_segment,
                sampling_rate=16000,
                return_tensors="pt",
                padding=True
            )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with self.torch.no_grad():
                outputs = self.model(**inputs)
                # æ—¶é—´ç»´åº¦å¹³å‡æ± åŒ–
                embeddings = outputs.last_hidden_state.mean(dim=1)
            
            embedding = embeddings.cpu().numpy()[0]
            return embedding
            
        except Exception as e:
            warnings.warn(f"åµŒå…¥æå–å¤±è´¥: {e}")
            return np.zeros(self.EMBEDDING_DIM)
    
    def cosine_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªåµŒå…¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
        
        Args:
            emb1: ç¬¬ä¸€ä¸ªåµŒå…¥
            emb2: ç¬¬äºŒä¸ªåµŒå…¥
        
        Returns:
            ç›¸ä¼¼åº¦å¾—åˆ† (0.0 åˆ° 1.0)
        """
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        if norm1 < 1e-8 or norm2 < 1e-8:
            return 0.5  # æ— æ•ˆåµŒå…¥è¿”å›ä¸­æ€§åˆ†æ•°
        
        emb1_norm = emb1 / norm1
        emb2_norm = emb2 / norm2
        
        # ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´ [-1, 1]
        similarity = np.dot(emb1_norm, emb2_norm)
        
        # è½¬æ¢åˆ° [0, 1] èŒƒå›´
        similarity = (similarity + 1.0) / 2.0
        
        return float(similarity)
    
    def score_segments(
        self,
        sliced_results: List[Dict],
        reference_audio_path: Optional[str] = None,
        reference_segments: Optional[List[Dict]] = None
    ) -> List[Dict]:
        """
        å¯¹å‘éŸ³ç‰‡æ®µè¿›è¡Œå£°å­¦è¯„åˆ†ã€‚
        
        Args:
            sliced_results: AudioSlicer è¾“å‡ºçš„åˆ‡ç‰‡ç»“æœ
            reference_audio_path: å¯é€‰çš„æ ‡å‡†éŸ³è·¯å¾„ï¼ˆæ—§æ¥å£ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
            reference_segments: å¯é€‰çš„æ ‡å‡†éŸ³åˆ‡ç‰‡åˆ—è¡¨ï¼ˆæ–°æ¥å£ï¼Œä¼˜å…ˆä½¿ç”¨ï¼‰
        
        Returns:
            æ·»åŠ å£°å­¦å¾—åˆ†çš„ç»“æœåˆ—è¡¨
        """
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ã€‚è¯·å…ˆè°ƒç”¨ load_model()")
        
        scored_results = []
        
        # ä¼˜å…ˆä½¿ç”¨å·²åˆ‡ç‰‡çš„æ ‡å‡†éŸ³
        reference_embeddings = None
        print(f"   ğŸ” æ£€æŸ¥ reference_segments: {reference_segments is not None}, é•¿åº¦: {len(reference_segments) if reference_segments else 0}")
        
        if reference_segments and len(reference_segments) > 0:
            # ç›´æ¥ä»åˆ‡ç‰‡æå–åµŒå…¥
            print(f"   âœ… ä½¿ç”¨æ ‡å‡†éŸ³åˆ‡ç‰‡æå–åµŒå…¥...")
            reference_embeddings = []
            for idx, seg in enumerate(reference_segments):
                ref_audio = seg.get("audio_segment", np.array([]))
                print(f"      [{idx}] {seg.get('char', '?')}: éŸ³é¢‘é•¿åº¦={len(ref_audio)}")
                ref_embedding = self.extract_embedding(ref_audio)
                reference_embeddings.append(ref_embedding)
            print(f"   âœ… æå–äº† {len(reference_embeddings)} ä¸ªæ ‡å‡†éŸ³åµŒå…¥")
        elif reference_audio_path:
            # å…¼å®¹æ—§æ¥å£ï¼šä»æ–‡ä»¶åˆ‡ç‰‡ï¼ˆä¸æ¨èï¼‰
            print(f"   âš ï¸ ä½¿ç”¨æ—§æ¥å£ä»æ–‡ä»¶åˆ‡ç‰‡")
            reference_embeddings = self._extract_reference_embeddings(
                reference_audio_path,
                sliced_results
            )
        else:
            print(f"   âš ï¸ æ— æ ‡å‡†éŸ³ï¼Œä½¿ç”¨å¯å‘å¼è¯„åˆ†")
        
        for i, item in enumerate(sliced_results):
            audio_segment = item.get("audio_segment", np.array([]))
            audio_length = len(audio_segment)
            
            # æå–ç”¨æˆ·åµŒå…¥
            user_embedding = self.extract_embedding(audio_segment)
            
            # è®¡ç®—å¾—åˆ†
            if reference_embeddings and i < len(reference_embeddings):
                # æœ‰æ ‡å‡†éŸ³ï¼šä½¿ç”¨åµŒå…¥ç›¸ä¼¼åº¦
                ref_embedding = reference_embeddings[i]
                similarity = self.cosine_similarity(user_embedding, ref_embedding)
                
                # è°ƒè¯•æ—¥å¿—
                char_name = item.get('char', f'[{i}]')
                print(f"   {char_name}: ä½™å¼¦ç›¸ä¼¼åº¦={similarity:.4f}", end="")
                
                # ç›¸ä¼¼åº¦æ˜ å°„åˆ°è¯„åˆ†ï¼ˆæé«˜å¯¹å¥½å‘éŸ³çš„åŒºåˆ†åº¦ï¼‰
                acoustic_score = self._similarity_to_score(similarity)
                print(f" â†’ å£°å­¦å¾—åˆ†={acoustic_score:.4f}")
            else:
                # æ— æ ‡å‡†éŸ³ï¼šä½¿ç”¨å¯å‘å¼è¯„åˆ†
                acoustic_score = self._heuristic_score(user_embedding, audio_segment)
            
            # çŸ­éŸ³é¢‘æƒ©ç½š
            if audio_length < self.MIN_AUDIO_LENGTH:
                length_ratio = audio_length / self.MIN_AUDIO_LENGTH
                acoustic_score *= max(0.5, length_ratio)
            
            # æ·»åŠ ç»“æœ
            result = item.copy()
            result["acoustic_score"] = float(acoustic_score)
            result["audio_length"] = audio_length
            scored_results.append(result)
        
        return scored_results
    
    def _similarity_to_score(self, similarity: float) -> float:
        """
        å°†ä½™å¼¦ç›¸ä¼¼åº¦æ˜ å°„åˆ°è¯„åˆ†ã€‚
        
        WavLM åµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦ç‰¹ç‚¹ï¼š
        - ç›¸åŒéŸ³é¢‘: ~0.99-1.0
        - ç›¸ä¼¼å‘éŸ³: ~0.85-0.95
        - ä¸åŒå‘éŸ³: ~0.6-0.8
        - å®Œå…¨ä¸åŒ: <0.6
        
        Args:
            similarity: ä½™å¼¦ç›¸ä¼¼åº¦ (0-1)
        
        Returns:
            è¯„åˆ† (0-1)
        """
        # æ›´å®½å®¹çš„æ˜ å°„ï¼š
        # similarity >= 0.9 -> score >= 0.95 (ä¼˜ç§€)
        # similarity >= 0.8 -> score >= 0.85 (è‰¯å¥½)
        # similarity >= 0.7 -> score >= 0.70 (åŠæ ¼)
        # similarity < 0.7 -> score < 0.70 (éœ€æ”¹è¿›)
        
        if similarity >= 0.9:
            # ä¼˜ç§€åŒºé—´ï¼š0.9-1.0 -> 0.95-1.0
            score = 0.95 + (similarity - 0.9) * 0.5
        elif similarity >= 0.8:
            # è‰¯å¥½åŒºé—´ï¼š0.8-0.9 -> 0.85-0.95
            score = 0.85 + (similarity - 0.8) * 1.0
        elif similarity >= 0.7:
            # åŠæ ¼åŒºé—´ï¼š0.7-0.8 -> 0.70-0.85
            score = 0.70 + (similarity - 0.7) * 1.5
        elif similarity >= 0.6:
            # å¾…æ”¹è¿›ï¼š0.6-0.7 -> 0.55-0.70
            score = 0.55 + (similarity - 0.6) * 1.5
        else:
            # è¾ƒå·®ï¼š0-0.6 -> 0.3-0.55
            score = 0.3 + similarity * 0.42
        
        return min(1.0, max(0.0, score))
    
    def _extract_reference_embeddings(
        self,
        reference_audio_path: str,
        sliced_results: List[Dict]
    ) -> List[np.ndarray]:
        """
        ä»æ ‡å‡†éŸ³ä¸­æå–å¯¹åº”ç‰‡æ®µçš„åµŒå…¥ã€‚
        
        Args:
            reference_audio_path: æ ‡å‡†éŸ³è·¯å¾„
            sliced_results: åŒ…å«æ—¶é—´æˆ³çš„åˆ‡ç‰‡ç»“æœ
        
        Returns:
            å‚è€ƒåµŒå…¥åˆ—è¡¨
        """
        try:
            import librosa
        except ImportError:
            raise RuntimeError("librosa ä¸å¯ç”¨")
        
        try:
            ref_audio, _ = librosa.load(reference_audio_path, sr=16000, mono=True)
        except Exception as e:
            warnings.warn(f"åŠ è½½æ ‡å‡†éŸ³å¤±è´¥: {e}")
            return []
        
        ref_embeddings = []
        
        for item in sliced_results:
            start_time = item.get("start", 0)
            end_time = item.get("end", 0)
            
            start_sample = int(start_time * 16000)
            end_sample = int(end_time * 16000)
            
            start_sample = max(0, min(start_sample, len(ref_audio)))
            end_sample = max(0, min(end_sample, len(ref_audio)))
            
            if end_sample > start_sample:
                ref_segment = ref_audio[start_sample:end_sample]
            else:
                ref_segment = np.array([])
            
            ref_embedding = self.extract_embedding(ref_segment)
            ref_embeddings.append(ref_embedding)
        
        return ref_embeddings
    
    def _heuristic_score(
        self, 
        embedding: np.ndarray, 
        audio_segment: np.ndarray
    ) -> float:
        """
        æ— æ ‡å‡†éŸ³æ—¶çš„å¯å‘å¼è¯„åˆ†ã€‚
        
        åŸºäºåµŒå…¥ç»Ÿè®¡é‡å’ŒéŸ³é¢‘ç‰¹å¾ã€‚
        
        Args:
            embedding: éŸ³é¢‘åµŒå…¥
            audio_segment: åŸå§‹éŸ³é¢‘
        
        Returns:
            å¯å‘å¼è¯„åˆ† (0-1)
        """
        scores = []
        
        # 1. åµŒå…¥å‘é‡çš„æœ‰æ•ˆæ€§
        magnitude = np.linalg.norm(embedding)
        variance = np.var(embedding)
        
        # æœ‰æ•ˆåµŒå…¥åº”è¯¥æœ‰ä¸€å®šçš„é‡çº§å’Œæ–¹å·®
        # ç»éªŒå€¼ï¼šæ­£å¸¸è¯­éŸ³åµŒå…¥çš„ magnitude çº¦ 15-25ï¼Œvariance çº¦ 0.05-0.15
        mag_score = min(1.0, magnitude / 20.0)
        var_score = min(1.0, variance / 0.1)
        
        scores.append(0.5 * mag_score + 0.3 * var_score)
        
        # 2. éŸ³é¢‘èƒ½é‡
        if len(audio_segment) > 0:
            rms = np.sqrt(np.mean(audio_segment ** 2))
            # æ­£å¸¸è¯­éŸ³ RMS çº¦ 0.01-0.1
            energy_score = min(1.0, rms * 20)
            scores.append(energy_score * 0.2)
        
        # ç»¼åˆå¾—åˆ†
        score = sum(scores)
        
        # æ— æ ‡å‡†éŸ³æ—¶ï¼Œç»™äºˆè¾ƒä¿å®ˆçš„è¯„åˆ†ï¼ˆé¿å…è™šé«˜ï¼‰
        # æ˜ å°„åˆ° 0.5-0.85 èŒƒå›´
        score = 0.5 + score * 0.35
        
        return min(1.0, max(0.0, score))
    
    def is_available(self) -> bool:
        """æ£€æŸ¥ WavLM æ˜¯å¦å¯ç”¨ã€‚"""
        return self.available