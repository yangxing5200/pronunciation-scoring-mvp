"""
Task 2: Audio Alignment - Enhanced Version with Boundary Refinement

å¢å¼ºç‰ˆéŸ³é¢‘å¯¹é½å™¨ï¼š
- ä½¿ç”¨ FunASR è·å–åˆå§‹æ—¶é—´æˆ³
- åŸºäºèƒ½é‡å’Œè¿‡é›¶ç‡ä¼˜åŒ–å­—ç¬¦è¾¹ç•Œ
- å‡å°‘ç›¸é‚»å­—ç¬¦çš„å£°éŸ³æ··å 
"""

from typing import List, Dict, Optional, Tuple
from pathlib import Path
import warnings
import numpy as np
import re


class BoundaryRefiner:
    """
    åŸºäºå£°å­¦ç‰¹å¾çš„è¾¹ç•Œä¼˜åŒ–å™¨ã€‚
    
    åœ¨ ASR è¿”å›çš„ç²—ç•¥æ—¶é—´æˆ³åŸºç¡€ä¸Šï¼Œä½¿ç”¨èƒ½é‡å’Œè¿‡é›¶ç‡æ¥ç²¾ç»†è°ƒæ•´è¾¹ç•Œã€‚
    """
    
    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate
        self.frame_size = 256  # 16ms @ 16kHz
        self.hop_size = 64     # 4ms @ 16kHz
        
        # è¾¹ç•Œæœç´¢èŒƒå›´ï¼ˆç§’ï¼‰
        self.search_range = 0.05  # 50ms
        
        # æœ€å°é—´éš”ï¼ˆç§’ï¼‰- é¿å…ç›¸é‚»å­—ç¬¦é‡å 
        self.min_gap = 0.02  # 20ms
    
    def compute_energy(self, audio: np.ndarray) -> np.ndarray:
        """è®¡ç®—çŸ­æ—¶èƒ½é‡ã€‚"""
        n_frames = (len(audio) - self.frame_size) // self.hop_size + 1
        energy = np.zeros(n_frames)
        
        for i in range(n_frames):
            start = i * self.hop_size
            frame = audio[start:start + self.frame_size]
            energy[i] = np.sum(frame ** 2) / self.frame_size
        
        return energy
    
    def compute_zcr(self, audio: np.ndarray) -> np.ndarray:
        """è®¡ç®—è¿‡é›¶ç‡ã€‚"""
        n_frames = (len(audio) - self.frame_size) // self.hop_size + 1
        zcr = np.zeros(n_frames)
        
        for i in range(n_frames):
            start = i * self.hop_size
            frame = audio[start:start + self.frame_size]
            # è®¡ç®—ç¬¦å·å˜åŒ–æ¬¡æ•°
            signs = np.sign(frame)
            zcr[i] = np.sum(np.abs(np.diff(signs))) / (2 * self.frame_size)
        
        return zcr
    
    def compute_spectral_flux(self, audio: np.ndarray) -> np.ndarray:
        """è®¡ç®—é¢‘è°±å˜åŒ–ç‡ï¼ˆç”¨äºæ£€æµ‹éŸ³ç´ è¾¹ç•Œï¼‰ã€‚"""
        try:
            import librosa
            # è®¡ç®—çŸ­æ—¶å‚…é‡Œå¶å˜æ¢
            stft = librosa.stft(audio, n_fft=512, hop_length=self.hop_size)
            magnitude = np.abs(stft)
            
            # è®¡ç®—å¸§é—´å·®å¼‚
            flux = np.zeros(magnitude.shape[1])
            for i in range(1, magnitude.shape[1]):
                diff = magnitude[:, i] - magnitude[:, i-1]
                flux[i] = np.sum(np.maximum(diff, 0))  # åªè€ƒè™‘æ­£å‘å˜åŒ–
            
            return flux
        except:
            # librosa ä¸å¯ç”¨ï¼Œè¿”å›ç©ºæ•°ç»„
            return np.array([])
    
    def find_boundary(
        self,
        audio: np.ndarray,
        initial_time: float,
        is_start: bool,
        prev_end: Optional[float] = None,
        next_start: Optional[float] = None
    ) -> float:
        """
        å¯»æ‰¾æœ€ä½³è¾¹ç•Œç‚¹ã€‚
        
        Args:
            audio: å®Œæ•´éŸ³é¢‘
            initial_time: ASR è¿”å›çš„åˆå§‹æ—¶é—´ç‚¹
            is_start: æ˜¯å¦æ˜¯èµ·å§‹è¾¹ç•Œï¼ˆå¦åˆ™æ˜¯ç»“æŸè¾¹ç•Œï¼‰
            prev_end: ä¸Šä¸€ä¸ªå­—ç¬¦çš„ç»“æŸæ—¶é—´
            next_start: ä¸‹ä¸€ä¸ªå­—ç¬¦çš„å¼€å§‹æ—¶é—´
        
        Returns:
            ä¼˜åŒ–åçš„æ—¶é—´ç‚¹
        """
        initial_sample = int(initial_time * self.sample_rate)
        search_samples = int(self.search_range * self.sample_rate)
        
        # ç¡®å®šæœç´¢èŒƒå›´
        if is_start:
            # èµ·å§‹è¾¹ç•Œï¼šå‘å‰æœç´¢
            search_start = max(0, initial_sample - search_samples)
            search_end = min(len(audio), initial_sample + search_samples // 2)
            
            # ä¸èƒ½æ—©äºä¸Šä¸€ä¸ªå­—ç¬¦çš„ç»“æŸ
            if prev_end is not None:
                min_sample = int((prev_end + self.min_gap) * self.sample_rate)
                search_start = max(search_start, min_sample)
        else:
            # ç»“æŸè¾¹ç•Œï¼šå‘åæœç´¢
            search_start = max(0, initial_sample - search_samples // 2)
            search_end = min(len(audio), initial_sample + search_samples)
            
            # ä¸èƒ½æ™šäºä¸‹ä¸€ä¸ªå­—ç¬¦çš„å¼€å§‹
            if next_start is not None:
                max_sample = int((next_start - self.min_gap) * self.sample_rate)
                search_end = min(search_end, max_sample)
        
        if search_end <= search_start:
            return initial_time
        
        # æå–æœç´¢åŒºåŸŸçš„ç‰¹å¾
        segment = audio[search_start:search_end]
        if len(segment) < self.frame_size:
            return initial_time
        
        energy = self.compute_energy(segment)
        zcr = self.compute_zcr(segment)
        
        if len(energy) == 0:
            return initial_time
        
        # å½’ä¸€åŒ–
        energy_norm = (energy - energy.min()) / (energy.max() - energy.min() + 1e-8)
        zcr_norm = (zcr - zcr.min()) / (zcr.max() - zcr.min() + 1e-8)
        
        # è®¡ç®—è¾¹ç•Œåˆ†æ•°
        # å¯¹äºèµ·å§‹è¾¹ç•Œï¼šå¯»æ‰¾èƒ½é‡ä»ä½åˆ°é«˜çš„è·³å˜ç‚¹
        # å¯¹äºç»“æŸè¾¹ç•Œï¼šå¯»æ‰¾èƒ½é‡ä»é«˜åˆ°ä½çš„è·³å˜ç‚¹
        if is_start:
            # èµ·å§‹è¾¹ç•Œï¼šä½èƒ½é‡ + èƒ½é‡ä¸Šå‡
            boundary_score = (1 - energy_norm[:-1]) * 0.5 + np.diff(energy_norm) * 0.5
        else:
            # ç»“æŸè¾¹ç•Œï¼šèƒ½é‡ä¸‹é™
            boundary_score = -np.diff(energy_norm) * 0.7 + (1 - energy_norm[:-1]) * 0.3
        
        if len(boundary_score) == 0:
            return initial_time
        
        # æ‰¾åˆ°æœ€ä½³è¾¹ç•Œç‚¹
        best_frame = np.argmax(boundary_score)
        best_sample = search_start + best_frame * self.hop_size
        best_time = best_sample / self.sample_rate
        
        # ç¡®ä¿è¾¹ç•Œåœ¨åˆç†èŒƒå›´å†…
        if is_start and prev_end is not None:
            best_time = max(best_time, prev_end + self.min_gap)
        if not is_start and next_start is not None:
            best_time = min(best_time, next_start - self.min_gap)
        
        return best_time
    
    def refine_boundaries(
        self,
        audio: np.ndarray,
        timestamps: List[Dict]
    ) -> List[Dict]:
        """
        ä¼˜åŒ–æ‰€æœ‰å­—ç¬¦çš„è¾¹ç•Œã€‚
        
        Args:
            audio: å®Œæ•´éŸ³é¢‘
            timestamps: åˆå§‹æ—¶é—´æˆ³åˆ—è¡¨ [{'char': 'ä½ ', 'start': 0.1, 'end': 0.3}, ...]
        
        Returns:
            ä¼˜åŒ–åçš„æ—¶é—´æˆ³åˆ—è¡¨
        """
        if len(timestamps) == 0:
            return timestamps
        
        refined = []
        
        for i, ts in enumerate(timestamps):
            # è·å–ç›¸é‚»å­—ç¬¦çš„æ—¶é—´ä¿¡æ¯
            prev_end = refined[-1]['end'] if i > 0 else None
            next_start = timestamps[i + 1]['start'] if i < len(timestamps) - 1 else None
            
            # ä¼˜åŒ–èµ·å§‹è¾¹ç•Œ
            new_start = self.find_boundary(
                audio,
                ts['start'],
                is_start=True,
                prev_end=prev_end
            )
            
            # ä¼˜åŒ–ç»“æŸè¾¹ç•Œ
            new_end = self.find_boundary(
                audio,
                ts['end'],
                is_start=False,
                next_start=next_start
            )
            
            # ç¡®ä¿ start < end
            if new_end <= new_start:
                new_end = new_start + 0.05  # æœ€å° 50ms
            
            refined.append({
                **ts,
                'start': new_start,
                'end': new_end,
                'original_start': ts['start'],
                'original_end': ts['end']
            })
        
        return refined
    
    def remove_overlap(
        self,
        timestamps: List[Dict],
        audio_duration: float
    ) -> List[Dict]:
        """
        ç§»é™¤ç›¸é‚»å­—ç¬¦çš„æ—¶é—´é‡å ã€‚
        
        Args:
            timestamps: æ—¶é—´æˆ³åˆ—è¡¨
            audio_duration: éŸ³é¢‘æ€»æ—¶é•¿
        
        Returns:
            æ— é‡å çš„æ—¶é—´æˆ³åˆ—è¡¨
        """
        if len(timestamps) <= 1:
            return timestamps
        
        result = []
        
        for i, ts in enumerate(timestamps):
            start = ts['start']
            end = ts['end']
            
            # æ£€æŸ¥ä¸ä¸Šä¸€ä¸ªå­—ç¬¦çš„é‡å 
            if i > 0 and start < result[-1]['end']:
                # å–ä¸­ç‚¹ä½œä¸ºåˆ†ç•Œ
                mid = (result[-1]['start'] + end) / 2
                # ç¡®ä¿æ¯ä¸ªå­—ç¬¦è‡³å°‘æœ‰ 30ms
                min_duration = 0.03
                
                if mid - result[-1]['start'] >= min_duration and end - mid >= min_duration:
                    result[-1]['end'] = mid - 0.005  # 5ms é—´éš”
                    start = mid + 0.005
                else:
                    # æ— æ³•åˆ†å‰²ï¼Œä¿æŒåŸæ ·
                    start = result[-1]['end'] + 0.01
            
            # ç¡®ä¿ä¸è¶…å‡ºéŸ³é¢‘è¾¹ç•Œ
            end = min(end, audio_duration)
            start = min(start, audio_duration - 0.01)
            
            result.append({
                **ts,
                'start': start,
                'end': end
            })
        
        return result


class ChineseAudioAlignerEnhanced:
    """
    å¢å¼ºç‰ˆä¸­æ–‡éŸ³é¢‘å¯¹é½å™¨ã€‚
    
    åœ¨ FunASR å¯¹é½åŸºç¡€ä¸Šæ·»åŠ è¾¹ç•Œä¼˜åŒ–ã€‚
    """
    
    # å­—ç¬¦æ®µæœ€å°/æœ€å¤§æ—¶é•¿ï¼ˆç§’ï¼‰
    MIN_CHAR_DURATION = 0.05  # 50ms
    MAX_CHAR_DURATION = 1.0   # 1s
    DEFAULT_CHAR_DURATION = 0.25  # 250ms
    
    def __init__(self, device: Optional[str] = None):
        """åˆå§‹åŒ–å¯¹é½å™¨ã€‚"""
        self.device = device
        self.funasr_available = False
        self.funasr_model = None
        self.boundary_refiner = BoundaryRefiner()
        
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        try:
            import torch
            if device is None:
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")
        except ImportError:
            self.device = "cpu"
        
        # æ£€æµ‹ FunASR
        try:
            from funasr import AutoModel
            self.AutoModel = AutoModel
            self.funasr_available = True
            print("âœ… FunASR å¯ç”¨")
        except ImportError:
            warnings.warn("FunASR ä¸å¯ç”¨ã€‚å®‰è£…å‘½ä»¤: pip install funasr modelscope")
    
    def load_models(self, model_size: str = "base"):
        """åŠ è½½ ASR æ¨¡å‹ã€‚"""
        if self.funasr_available:
            try:
                print("ğŸ“¥ åŠ è½½ FunASR Paraformer æ¨¡å‹...")
                self.funasr_model = self.AutoModel(
                    model="paraformer-zh",
                    model_revision="v2.0.4",
                    vad_model="fsmn-vad",
                    vad_model_revision="v2.0.4",
                    punc_model="ct-punc",
                    punc_model_revision="v2.0.4",
                    device=self.device
                )
                print(f"âœ… FunASR æ¨¡å‹åŠ è½½å®Œæˆ")
            except Exception as e:
                warnings.warn(f"FunASR åŠ è½½å¤±è´¥: {e}")
                self.funasr_available = False
    
    def align_audio(
        self,
        audio_path: str,
        pinyin_sequence: List[Dict[str, str]],
        refine_boundaries: bool = True
    ) -> List[Dict]:
        """
        å¯¹é½éŸ³é¢‘ä¸æ‹¼éŸ³åºåˆ—ã€‚
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            pinyin_sequence: æ‹¼éŸ³åºåˆ—
            refine_boundaries: æ˜¯å¦ä¼˜åŒ–è¾¹ç•Œ
        
        Returns:
            å¯¹é½ç»“æœåˆ—è¡¨
        """
        # åŠ è½½éŸ³é¢‘
        audio, audio_duration = self._load_audio(audio_path)
        
        if self.funasr_model is None:
            return self._create_fallback_alignment(pinyin_sequence, audio_duration)
        
        # FunASR è¯†åˆ«
        try:
            result = self.funasr_model.generate(
                input=audio_path,
                batch_size_s=300,
                return_raw_text=False,
            )
        except Exception as e:
            warnings.warn(f"FunASR è¯†åˆ«å¤±è´¥: {e}")
            return self._create_fallback_alignment(pinyin_sequence, audio_duration)
        
        if not result:
            return self._create_fallback_alignment(pinyin_sequence, audio_duration)
        
        # è§£æç»“æœ
        funasr_result = result[0] if isinstance(result, list) else result
        recognized_text = funasr_result.get('text', '')
        timestamps = funasr_result.get('timestamp', [])
        
        print(f"ğŸ¤ è¯†åˆ«ç»“æœ: {recognized_text}")
        print(f"   æœŸæœ›æ–‡æœ¬: {''.join([item['char'] for item in pinyin_sequence])}")
        
        # æ„å»ºåˆå§‹æ—¶é—´æˆ³
        char_timestamps = self._build_char_timestamps(
            recognized_text, timestamps, audio_duration
        )
        
        # ä¸æœŸæœ›åºåˆ—å¯¹é½
        aligned_results = self._align_with_expected_sequence(
            char_timestamps, pinyin_sequence, audio_duration
        )
        
        # è¾¹ç•Œä¼˜åŒ–
        if refine_boundaries and len(audio) > 0:
            print("ğŸ”§ æ­£åœ¨ä¼˜åŒ–å­—ç¬¦è¾¹ç•Œ...")
            aligned_results = self.boundary_refiner.refine_boundaries(
                audio, aligned_results
            )
            aligned_results = self.boundary_refiner.remove_overlap(
                aligned_results, audio_duration
            )
            print("âœ… è¾¹ç•Œä¼˜åŒ–å®Œæˆ")
        
        # åå¤„ç†
        aligned_results = self._postprocess_timestamps(aligned_results, audio_duration)
        
        return aligned_results
    
    def _load_audio(self, audio_path: str) -> Tuple[np.ndarray, float]:
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶ã€‚"""
        try:
            import librosa
            audio, sr = librosa.load(audio_path, sr=16000, mono=True)
            duration = len(audio) / sr
            return audio, duration
        except Exception as e:
            warnings.warn(f"åŠ è½½éŸ³é¢‘å¤±è´¥: {e}")
            return np.array([]), 5.0
    
    def _build_char_timestamps(
        self,
        recognized_text: str,
        timestamps: List,
        audio_duration: float
    ) -> List[Dict]:
        """æ„å»ºå­—ç¬¦æ—¶é—´æˆ³ã€‚"""
        char_timestamps = []
        
        if timestamps and len(timestamps) == len(recognized_text):
            for i, char in enumerate(recognized_text):
                if re.match(r'[\u4e00-\u9fff]', char):
                    ts = timestamps[i]
                    if isinstance(ts, (list, tuple)) and len(ts) >= 2:
                        start = ts[0] / 1000.0
                        end = ts[1] / 1000.0
                        char_timestamps.append({
                            'char': char,
                            'start': start,
                            'end': end,
                            'score': 0.9
                        })
        elif timestamps:
            # æ—¶é—´æˆ³æ•°é‡ä¸åŒ¹é…
            chinese_chars = re.findall(r'[\u4e00-\u9fff]', recognized_text)
            if len(timestamps) == len(chinese_chars):
                for i, char in enumerate(chinese_chars):
                    ts = timestamps[i]
                    if isinstance(ts, (list, tuple)) and len(ts) >= 2:
                        char_timestamps.append({
                            'char': char,
                            'start': ts[0] / 1000.0,
                            'end': ts[1] / 1000.0,
                            'score': 0.85
                        })
            else:
                # å‡åŒ€åˆ†é…
                char_duration = audio_duration / max(1, len(chinese_chars))
                for i, char in enumerate(chinese_chars):
                    char_timestamps.append({
                        'char': char,
                        'start': i * char_duration,
                        'end': (i + 1) * char_duration,
                        'score': 0.5
                    })
        
        return char_timestamps
    
    def _align_with_expected_sequence(
        self,
        char_timestamps: List[Dict],
        pinyin_sequence: List[Dict],
        audio_duration: float
    ) -> List[Dict]:
        """å°†è¯†åˆ«ç»“æœä¸æœŸæœ›åºåˆ—å¯¹é½ã€‚"""
        aligned_results = []
        
        expected_chars = [item['char'] for item in pinyin_sequence]
        recognized_chars = [item['char'] for item in char_timestamps]
        
        # ç®€å•åŒ¹é…
        for i, expected in enumerate(pinyin_sequence):
            char = expected['char']
            pinyin = expected['pinyin']
            
            # åœ¨è¯†åˆ«ç»“æœä¸­æŸ¥æ‰¾
            matched = None
            for j, ts in enumerate(char_timestamps):
                if ts['char'] == char:
                    matched = ts
                    break
            
            if matched:
                aligned_results.append({
                    'char': char,
                    'pinyin': pinyin,
                    'start': matched['start'],
                    'end': matched['end'],
                    'score': matched['score']
                })
            else:
                # æœªåŒ¹é…ï¼Œä¼°è®¡æ—¶é—´æˆ³
                ts = self._estimate_timestamp(
                    i, len(pinyin_sequence), char_timestamps, audio_duration
                )
                aligned_results.append({
                    'char': char,
                    'pinyin': pinyin,
                    'start': ts['start'],
                    'end': ts['end'],
                    'score': 0.3
                })
        
        return aligned_results
    
    def _estimate_timestamp(
        self,
        index: int,
        total_chars: int,
        char_timestamps: List[Dict],
        audio_duration: float
    ) -> Dict:
        """ä¼°è®¡ç¼ºå¤±å­—ç¬¦çš„æ—¶é—´æˆ³ã€‚"""
        if char_timestamps:
            all_starts = [ts['start'] for ts in char_timestamps]
            all_ends = [ts['end'] for ts in char_timestamps]
            total_start = min(all_starts)
            total_end = max(all_ends)
            char_duration = (total_end - total_start) / max(1, total_chars)
            return {
                'start': total_start + index * char_duration,
                'end': total_start + (index + 1) * char_duration
            }
        
        char_duration = audio_duration / max(1, total_chars)
        return {
            'start': index * char_duration,
            'end': (index + 1) * char_duration
        }
    
    def _postprocess_timestamps(
        self,
        aligned_results: List[Dict],
        audio_duration: float
    ) -> List[Dict]:
        """åå¤„ç†æ—¶é—´æˆ³ã€‚"""
        if not aligned_results:
            return aligned_results
        
        processed = []
        
        for i, item in enumerate(aligned_results):
            start = max(0, item['start'])
            end = max(0, item['end'])
            
            if end <= start:
                end = start + self.MIN_CHAR_DURATION
            
            duration = end - start
            if duration < self.MIN_CHAR_DURATION:
                end = start + self.MIN_CHAR_DURATION
            elif duration > self.MAX_CHAR_DURATION:
                end = start + self.MAX_CHAR_DURATION
            
            if end > audio_duration:
                end = audio_duration
                if start >= end:
                    start = max(0, end - self.MIN_CHAR_DURATION)
            
            processed.append({
                **item,
                'start': start,
                'end': end
            })
        
        return processed
    
    def _create_fallback_alignment(
        self,
        pinyin_sequence: List[Dict],
        audio_duration: float
    ) -> List[Dict]:
        """åˆ›å»ºå¤‡ç”¨å¯¹é½ï¼ˆå‡åŒ€åˆ†é…ï¼‰ã€‚"""
        n_chars = len(pinyin_sequence)
        if n_chars == 0:
            return []
        
        char_duration = audio_duration / n_chars
        char_duration = max(self.MIN_CHAR_DURATION, min(char_duration, self.MAX_CHAR_DURATION))
        
        results = []
        for i, item in enumerate(pinyin_sequence):
            results.append({
                'char': item['char'],
                'pinyin': item['pinyin'],
                'start': i * char_duration,
                'end': (i + 1) * char_duration,
                'score': 0.3
            })
        
        return results
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨ã€‚"""
        return self.funasr_available
